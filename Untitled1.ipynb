{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Quality and Validation in ETL **"
      ],
      "metadata": {
        "id": "N9OzQboUkrGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1  . Data quality in ETL pipelines refers to the degree to which data meets\n",
        "     the requirements of accuracy , completeness consistancy ,timeliness and\n",
        "     relevance for its intended use , its is more than just data cleaning because it encompasses a holistic set of processes validation profiling enrichment and govarnance that ensure data analysis and decision making throughout the entire pipeline ,not just removing errors .\n",
        "\n",
        "2  . poor dtat quality leads to misleading dasboards and incorrect decisions\n",
        "     because flawed data propagates through analytic producing inaccurate metrics skewed visualizations and based insights.causing business losses or strategic errors.\n",
        "\n",
        "3  . Source system redundancy - multiple input contain the same information.\n",
        "     Merge/join errors - improper joins create repeated entries .\n",
        "     Incomplete deduplication - lack of unique key enforcement or missing cleanup steps.\n",
        "\n",
        "4  . Exact duplicates are identical records with every field matching\n",
        "     precisely .\n",
        "     partial duplicates shere some near-identical records that require similarity algorithems to detect because value may have minor variation .\n",
        "\n",
        "5  . Prevents bad data from entering the target system reducing storage of\n",
        "     erroneous information .\n",
        "     Enables early error correction saving compute resources and avoiding\n",
        "     costly post-load fixes .\n",
        "     Improves pipeline efficiency by catching issues before final load ensuring the data warehouse contains reliable usable data.\n",
        "\n",
        "6  . Business rules are constraints that enforce data validity and\n",
        "     consistency they help validate data accuracy by checking entires against predefined conditions preventing errors or inconsistencies.\n",
        "\n",
        "7  . select\n",
        "          customer_id ,product_id ,txn_data ,txn_amount,\n",
        "          COUNT(*) AS duplicate_count\n",
        "      FROM\n",
        "          sale_transations\n",
        "      GROUP BY\n",
        "             customer_id ,product_id ,txn_date,txn_amount\n",
        "      HAVING\n",
        "           COUNT(*) >1 ;\n",
        "     \n",
        "8  . Select distinct sales_transaction.customer_id\n",
        "     FROM sale_transaction\n",
        "     WHERE sale_transactions.customer_id NOT IN(\n",
        "     SELECT customer_id from customers_master\n",
        "     );"
      ],
      "metadata": {
        "id": "yXyKKlyGkz2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cR42WcK9wWCA"
      }
    }
  ]
}